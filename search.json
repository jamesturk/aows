[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Art of Web Scraping",
    "section": "",
    "text": "Introduction\nI’ve found that web scraping can be quite polarizing among developers. A lot of people hate it, finding it frustrating and tedious. On the other hand, many get a sense of satisfaction out of tackling the challenges it presents and the end result of having machine-readable data where it didn’t exist before.\nFor thirteen years (2009-2022), I led the Open States project. Open States is a public resource which provides legislative data for all 50 states (and DC and Puerto Rico!) that updates regularly throughout the day. The project relies on around 200 custom scrapers, and then uses that data to power a variety of services including a public API and website. Millions of individuals have used Open States over the years to find their representatives & track legislation, and the data is widely used by researchers, journalists, and nonprofits.\nSo, it makes sense that I’m definitely in the latter camp, I enjoy writing web scrapers & seeing the difference the final product can make. I also believe that a lot of people that hate web scraping hate it because their experiences were terrible. The commonly used libraries are not ergonomic, most scraper code is ugly and hard to maintain, and the write-run-wait-debug cycle is slow and painful. Additionally, many people approach web scraping with the same mindset they use for writing web applications or other software, which no doubt leads to a frustrating experience.\nI believe these factors are exacerbated by the fact that a lot of web scrapers are built as throwaway one-offs (whether or not they actually are is a different story), and as a result not a lot of thought is put into their design. This makes sense, but creates a sort of feedback loop where most scraper code is fragile and hard to maintain, so people looking to write more reliable scrapers wind up learning from code that is fragile and hard to maintain. Over the years I onboarded dozens of new contributors, and often wished for better resources on web scraping that reflected best practices. My hope is that this resource can help to fill that gap.\nGiven the way Data Engineering has taken off as a field in recent years, I think we’re overdue to take a second look at web scraping as a discipline. The scrapers are often the most fragile, least maintainable part of the pipeline. We’ll take a look at why that is, and how we can improve it."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Art of Web Scraping",
    "section": "About This Book",
    "text": "About This Book\n\n\n\n\n\n\nNote\n\n\n\nThis is a work in progress, the table of contents is aspirational.\nIf you’re interested in knowing when new chapters are ready, you can follow me on Mastodon.\nI’ve also set up a mailing list for updates. I’ll send out an email when new chapters are ready.\n\n\nOne of the goals of this project is to provide a resource for people looking to write reliable scrapers. While code examples will be in Python, many of the concepts should be applicable regardless of language. Part 1 in particular will focus on high-level concepts and design patterns, and be broadly applicable.\nPart 2 is explicitly focused on the Python scraping ecosystem. Whatever language you use, picking the right tools is essential. For a lot of people 2-3 Python libraries seem like the end-all-be-all, but there are a lot of options out there, and I think it’s worth exploring them. We’ll also take a look at some oft-neglected parts of the scraping stack, like data validation and logging.\nRight now I’m also grouping more advanced topics into Part 3. That’s the least certain part of the plan right now. If you have suggestions of things you’d like to see covered, please let me know.\nPart 1: Understanding Web Scraping\nThis section aims to cover broad topics in web scraping. This isn’t particularly focused on Python, but rather on the general concepts of web scraping.\n\nWeb Scraping 101 - An overview of the basics of web scraping.\nScraping Philosophy - A discussion of the philosophy behind writing scrapers.\nBest Practices - Writing resilient & sustainable scrapers.\nEthical & Legal Guidelines - A discussion of ethical & legal guidelines for scraping.\n\nPart 2: Python Scraping Ecosystem\nThese chapters are focused on the Python scraping ecosystem. Each chapter will compare a few libraries, and discuss the pros & cons of each.\n\nMaking Requests - Various libraries for making HTTP requests.\nParsing HTML - Comparing various libraries for parsing HTML.\nOther Libraries - Other libraries that are useful for scraping.\n\nPart 3: Advanced Concepts\nThese chapters discuss more advanced topics in web scraping. Not every scraper will need to use these, but they’re useful to know about as your scrapers grow more advanced.\n\nDeploying Scrapers - A discussion of deploying scrapers.\n\nAppendices\nAppendices that will contain reference information useful to people that are writing scrapers.\n\nAppendix A: XPath & CSS Selectors"
  },
  {
    "objectID": "101.html#what-is-web-scraping",
    "href": "101.html#what-is-web-scraping",
    "title": "1  Web Scraping 101",
    "section": "1.1 What is Web Scraping?",
    "text": "1.1 What is Web Scraping?"
  },
  {
    "objectID": "101.html#scraping-vs.-crawling",
    "href": "101.html#scraping-vs.-crawling",
    "title": "1  Web Scraping 101",
    "section": "1.2 Scraping vs. Crawling",
    "text": "1.2 Scraping vs. Crawling"
  },
  {
    "objectID": "101.html#parse-tree",
    "href": "101.html#parse-tree",
    "title": "1  Web Scraping 101",
    "section": "1.3 Parse Tree",
    "text": "1.3 Parse Tree"
  },
  {
    "objectID": "101.html#the-scraping-algorithm",
    "href": "101.html#the-scraping-algorithm",
    "title": "1  Web Scraping 101",
    "section": "1.4 The Scraping “Algorithm”",
    "text": "1.4 The Scraping “Algorithm”"
  },
  {
    "objectID": "101.html#the-browsers-role",
    "href": "101.html#the-browsers-role",
    "title": "1  Web Scraping 101",
    "section": "1.5 The Browser’s Role",
    "text": "1.5 The Browser’s Role"
  },
  {
    "objectID": "101.html#putting-it-all-together",
    "href": "101.html#putting-it-all-together",
    "title": "1  Web Scraping 101",
    "section": "1.6 Putting It All Together",
    "text": "1.6 Putting It All Together"
  },
  {
    "objectID": "philosophy.html#scrapers-are-ugly",
    "href": "philosophy.html#scrapers-are-ugly",
    "title": "2  Scraping Philosophy",
    "section": "2.1 Scrapers Are Ugly",
    "text": "2.1 Scrapers Are Ugly\nWeb scraping code is probably some of the ugliest code you’ll ever write.\nI always told new contributors, often less experienced developers coming in with an academic understanding of what clean code is, that they should understand that web scraping code can’t be more elegant than the site it is scraping. If the HTML page you are dealing with is full of weird edge cases, your code will necessarily be full of weird edge cases too.\nAlso, many web scrapers are written to be run once and thrown away, in which case, who cares if it’s ugly?\nFor Open States, we knew that our goal was to be able to run our scrapers continuously, to create an ongoing stream of legislative data updating our database.\nBut I’ve found scrapers are rarely as ephemeral as their authors intend. I’ve seen many scrapers someone wrote years ago to grab some data for a one-off project wound up being a core part of someone’s data pipeline long after the original author departed the team. The thought may horrify the original author, who assumed the code would be thrown away. With good enough tools, and the right philosophy, you can make this prospect slightly less terrifying.\nUltimately our core requirements shaped a lot of this philosophy:\n\nWe needed to be able to run our scrapers continuously, to create an ongoing stream of legislative data updating our database.\nOur scrapers were necessarily fragile, we were scraping specific legislative metadata, not just collecting full text. We couldn’t afford to be imprecise.\nAs an open source project dependent upon volunteers, we needed to be able to easily onboard new contributors.\nWe were scraping government websites, many of which had spotty uptime and were often slow to respond.\n\nWe built a lot of tools to help us with these goals, and over the years we built up a lot of knowledge about how to build reliable scrapers."
  },
  {
    "objectID": "philosophy.html#scrapers-are-fragile",
    "href": "philosophy.html#scrapers-are-fragile",
    "title": "2  Scraping Philosophy",
    "section": "2.2 Scrapers Are Fragile",
    "text": "2.2 Scrapers Are Fragile"
  },
  {
    "objectID": "philosophy.html#scrapers-are-tied-to-the-page-structure",
    "href": "philosophy.html#scrapers-are-tied-to-the-page-structure",
    "title": "2  Scraping Philosophy",
    "section": "2.3 Scrapers Are Tied to the Page Structure",
    "text": "2.3 Scrapers Are Tied to the Page Structure"
  },
  {
    "objectID": "philosophy.html#scrapers-stick-around",
    "href": "philosophy.html#scrapers-stick-around",
    "title": "2  Scraping Philosophy",
    "section": "2.4 Scrapers Stick Around",
    "text": "2.4 Scrapers Stick Around"
  },
  {
    "objectID": "philosophy.html#scrapers-are-living-things",
    "href": "philosophy.html#scrapers-are-living-things",
    "title": "2  Scraping Philosophy",
    "section": "2.5 Scrapers Are Living Things",
    "text": "2.5 Scrapers Are Living Things"
  },
  {
    "objectID": "philosophy.html#complementarity",
    "href": "philosophy.html#complementarity",
    "title": "2  Scraping Philosophy",
    "section": "2.6 Complementarity",
    "text": "2.6 Complementarity\nYou may be familiar with Heisenberg’s uncertainty principle, which states that the more precisely you measure the position of a particle, the less precisely you can measure its momentum, and vice versa. This is an example of a concept called complementarity.\nIn the context of scrapers, we can think of precision and robustness as being complementary. When selecting data from a page, you can either be precise and fragile, or you can be robust and imprecise. You can’t be both.\nAn example can make this more clear:\nLet’s say there is a deeply nested set of <div> tags on a page, something like this:\n<div class=\"container\">\n    <div class=\"section1\">\n        <div class=\"user\">\n            <h2>Sally Smith</h2>\n            <div>123 Main St</div>\n            <div>Boise</div>\n            <div>ID</div>\n            <div>12345</div>\n        </div>\n    </div>\n</div>\nYou could assume that <div class=\"user\"> contains the address, and select it like this:\naddress = \" \".join(page.xpath('//div[@class=\"user\"]/div/text()'))\nWhich would obtain \"123 Main St Boise ID 12345\".\nNow imagine that you encounter another user where there is a 5th div, and you realize that they add the phone number if available:.\n<div class=\"container\">\n    <div class=\"section1\">\n        <div class=\"user\">\n            <h2>Sally Smith</h2>\n            <div>123 Main St</div>\n            <div>Boise</div>\n            <div>ID</div>\n            <div>12345</div>\n            <div>555-555-5555</div>\n        </div>\n    </div>\n</div>\nYour code will now break, because it will select the phone number."
  },
  {
    "objectID": "best-practices.html#scraper-design",
    "href": "best-practices.html#scraper-design",
    "title": "3  Best Practices",
    "section": "3.1 Scraper Design",
    "text": "3.1 Scraper Design"
  },
  {
    "objectID": "best-practices.html#developer-ergonomics",
    "href": "best-practices.html#developer-ergonomics",
    "title": "3  Best Practices",
    "section": "3.2 Developer Ergonomics",
    "text": "3.2 Developer Ergonomics"
  },
  {
    "objectID": "best-practices.html#being-a-good-citizen",
    "href": "best-practices.html#being-a-good-citizen",
    "title": "3  Best Practices",
    "section": "3.3 Being a Good Citizen",
    "text": "3.3 Being a Good Citizen\n\n3.3.1 Scraping != Adversarial\n\n\n3.3.2 Identifying Yourself\n\n\n3.3.3 robots.txt\n\n\n3.3.4 Rate Limiting\n\nWhat is Rate Limiting?\n\n\nNaive Rate Limiting\n\n\nProper Rate Limiting\n\n\nEnhancements\n\n\n\n3.3.5 Caching\n\nHTTP Caching\n\n\nCaching for Scrapers\n\n\nCaveats"
  },
  {
    "objectID": "ethical-legal.html#ethics",
    "href": "ethical-legal.html#ethics",
    "title": "4  Ethics & Legal Issues",
    "section": "4.1 Ethics",
    "text": "4.1 Ethics\n\n4.1.1 Why Ethics?\n\n\n4.1.2 Can vs. Should\n\n\n4.1.3 “It’s out there anyway”"
  },
  {
    "objectID": "ethical-legal.html#legal-issues",
    "href": "ethical-legal.html#legal-issues",
    "title": "4  Ethics & Legal Issues",
    "section": "4.2 Legal Issues",
    "text": "4.2 Legal Issues\n\n4.2.1 Intellectual Property\n\n\n4.2.2 Privacy\n\n\n4.2.3 Circumvention\n\n\n4.2.4 The Future"
  },
  {
    "objectID": "parsing/index.html#introduction",
    "href": "parsing/index.html#introduction",
    "title": "6  Parsing HTML",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn this section we’ll be looking at four libraries for parsing HTML:\n\nBeautiful Soup\nlxml, specifically lxml.html\nselectolax\nand parsel, which is part of the Scrapy framework.\n\nBeautiful Soup\nWhen people talk about Python libraries for writing web scrapers, they immediately go to Beautiful Soup. Nearly 20 years old, it is one of the most well-established Python libraries out there. It is popular enough that I find that people are often surprised to learn there are viable alternatives.\nIf you look on sites like Stack Overflow, the conventional wisdom is that Beautiful Soup is the most flexible, while lxml is much faster. We’ll be taking a look to see if that wisdom holds up.\nIt is worth mentioning that Beautiful Soup 4 is a major departure from Beautiful Soup 3. So much so that when installing Beautiful Soup 4, you need to install the beautifulsoup4 package from PyPI.\nFurthermore, as of version 4, Beautiful Soup works as a wrapper around a number of different parsers. Its documentation explains how to pick a parser and offers some conventional wisdom about which you should pick. The default parser is html.parser, which is part of the Python standard library. You can also use lxml.html or html5lib. When it comes to evaluating the performance of Beautiful Soup, we’ll try all of these.\nlxml.html\nlxml is a Python library for parsing XML, and comes with lxml.html, a submodule specifically designed for handling HTML. The library is a wrapper around the libxml2 and libxslt C libraries. This means that it is very fast, but also requires that you have the C libraries installed on your system.\nUntil recently this was a bit of a challenge, but advances in Python packaging have made this process much easier in most environments.\nThe conventional wisdom, as mentioned before, is that lxml is fast but perhaps not as flexible as other options. Anecdotally that has not been my experience, Open States switched to lxml around the time of the somewhat fraught Beautiful Soup 4 transition and never really looked back.\nSelectolax\nThere’s also a much newer kid in town, Selectolax. It is a wrapper around a new open source HTML parsing library and claims to be even faster than lxml. It has a much smaller footprint than the other libraries, so it will be interesting to see how it stacks up against the more established libraries.\nParsel\nParsel is a library that is part of the popular Scrapy framework. It is a wrapper around lxml, but provides a very different interface for extracting data from HTML.\n\n\n\n\n\n\nNote\n\n\n\nThese libraries are not exact peers of one another. This is most notable with the way that Beautiful Soup and lxml allow you to use different parsers, and Parsel is itself a wrapper around lxml.\nWhile unusual combinations may exist, most projects will pick one of these and use it for all of their parsing needs, so we’ll be looking at them through that lens.\n\n\nIn this section, we’ll be taking a look at how these libraries stack up against one another.\nWe’ll try to answer questions like:\n\nWhich library offers the nicest developer experience?\nWith Beautiful Soup offering a wrapper around lxml, is there any reason to use lxml directly if you’re using Beautiful Soup?\nHave Python speed improvements negated much of lxml’s performance advantage?\nHow does Selectolax stack up against the more established libraries?\nHow much does the flexibility of the parsers differ in 2023? Is it worth the performance hit to use html5lib?\n\nTo start, we’ll take a look at the features that each offers before evaluating how they perform against one another."
  },
  {
    "objectID": "parsing/index.html#developer-experience",
    "href": "parsing/index.html#developer-experience",
    "title": "6  Parsing HTML",
    "section": "6.2 Developer Experience",
    "text": "6.2 Developer Experience\nWhen it comes to writing resilient scrapers, the developer experience is perhaps the most important dimension. A nice, clean, and consistent API is an important factor in the cleanliness & readability of your scraper code.\nWe’ll compare the experience by looking at a few aspects of the developer experience:\n\nFeatures\nComplexity\nDocumentation\nCommon Tasks Compared\n\nIt is also worth noting that all of the libraries are permissively licensed (either MIT or BSD) open source libraries. So on that front at least, the developer experience is the same.\n\n6.2.1 Feature Comparison\nThese libraries are all perfectly capable libraries, each provides HTML parsing as well as various selection mechanisms to extract content from the DOM.\nThe main differences among them are which methods they provide for selecting nodes, and how they handle text extraction:\n\n\nTable 6.1: Feature Comparison\n\n\nLibrary\nXPath\nCSS\nDOM Traversal API\nText Extraction\n\n\n\n\nlxml\n✅\n✳\n✅\n✅\n\n\nBeautiful Soup\n❌\n✅\n✅\n✅\n\n\nselectolax\n❌\n✅\n✅\n✅\n\n\nparsel\n✅\n✅\n❌\n❌\n\n\n\n\n✳ cssselect must be installed separately but augments lxml.html to provide CSS selector support.\n\nAttribute Access\nA common feature among all libraries is dictionary-like attribute access on nodes.\n\nExample: Extracting an attribute’s value\n\nhtml = \"\"\"<a data=\"foo\">\"\"\"\n\n\nlxmlBeautifulSoupSelectolaxparsel\n\n\n\nimport lxml.html\nroot = lxml.html.fromstring(html)\nnode = root.xpath('//a')[0]\nnode.attrib['data']\n\n'foo'\n\n\n\n\n\nimport bs4\nroot = bs4.BeautifulSoup(html, 'lxml')\nnode = root.find('a')\nnode['data']\n\n'foo'\n\n\n\n\n\nfrom selectolax.parser import HTMLParser\nroot = HTMLParser(html)\nnode = root.css_first('a')\nnode.attributes['data']\n\n'foo'\n\n\n\n\n\nfrom parsel import Selector\nroot = Selector(html)\nnode = root.xpath('//a')[0]\nnode.attrib['data']\n\n'foo'\n\n\n\n\n\n\n\n\nPluggable Parsers\nlxml and Beautiful Soup both have parsers and node APIs that are separate from one another. It is technically possible to use lxml.html’s parser with Beautiful Soup’s Node API, or vice versa. Additionally, selectolax allows choosing between two different backend parsers.\nThis isn’t going to factor into feature comparisons, since each supported parser is equally capable and we’ll be looking at speed & flexibility in other sections.\nHaving pluggable parsers is a nice feature, but as we’ll see in the rest of this comparison, it might not be as useful as it sounds.\n\n\nSelectors & DOM Traversal\nOnce HTML is parsed, there are many ways to actually select the nodes that you want to extract data from. As mentioned in earlier sections, using a selector language like XPath or CSS selectors is preferable, but sometimes you will need to fall back to traversing the DOM.\nlxml is an XML-first library, and as such supports the powerful XPath selection language. It also supports the ElementTree API, which is a DOM traversal API. It also supports CSS selectors, but you must install cssselect separately.\nparsel, mostly a wrapper around lxml, also supports XPath and CSS selectors treating both as equal. It does not however expose a DOM traversal API of its own.\nBeautiful Soup has a custom selector API, and also supports CSS selectors since 4.0. It also has dozens of methods for traversing the DOM.\nSelectolax is CSS-first, with no XPath support. It does also provide methods for directly traversing the DOM.\n\n\nText Extraction\nAll of these libraries provide a way to extract text from a node, but the methods differ.\n\nlxml provides a text_content() method that will return the text content of a node, including text from child nodes.\nBeautifulSoup similarly provides a .text property that will return the text content of a node, including text from child nodes.\nParsel does not actually provide a dedicated way to do this, you can use the .xpath('string()') method to get the text content of a node, including text from child nodes.\nselectolax provides a .text() method that will return the text content of a node, including text from child nodes.\n\n\nExample: Text Extraction\n<div class=\"content\">\n    This is some content that contains <em>a little</em> bit of markup.\n    <br />\n    We'll see that these inner tags create some <em>problems</em>,\n     which some libraries handle better than others.\n</div>\n\nBeautiful SouplxmlparselSelectolax\n\n\n\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'html.parser')\nsoup.text\n\n\"\\n\\n    This is some content that contains a little bit of markup.\\n    \\n    We'll see that these inner tags create some problems,\\n    which some libraries handle better than others.\\n\\n\"\n\n\n\n\n\nimport lxml.html\nroot = lxml.html.fromstring(html)\nroot.text_content()\n\n\"\\n    This is some content that contains a little bit of markup.\\n    \\n    We'll see that these inner tags create some problems,\\n    which some libraries handle better than others.\\n\"\n\n\n\n\n\nimport parsel\nsel = parsel.Selector(html)\nsel.xpath('string()').get()\n\n\"\\n    This is some content that contains a little bit of markup.\\n    \\n    We'll see that these inner tags create some problems,\\n    which some libraries handle better than others.\\n\"\n\n\n\n\n\nfrom selectolax.parser import HTMLParser\ntree = HTMLParser(html)\ntree.text()\n\n\"\\n    This is some content that contains a little bit of markup.\\n    \\n    We'll see that these inner tags create some problems,\\n    which some libraries handle better than others.\\n\\n\"\n\n\n\n\n\nSelectolax’s text extraction seems the most sophisticated. Its text method has convinient parameters to strip text and opt between including text from child nodes or not.\n\n\n\nparsel & lxml\nparsel at this point may seem to be lacking in features. We’ve seen that it does not support DOM traversal, or have a native method for extracting text from a node. It seems fair to note that you can access the underlying lxml object and use its methods, which provides one workaround.\nOf course, this is not a very clean solution requiring mixing of two APIs and would break if parsel ever switched to a different underlying library. (This has at least been proposed, but it is not clear it is likely.)\n\n\n\n6.2.2 Complexity\nAt their core, all of these APIs provide a method to parse a string of HTML, and then a node API where most of the work is done. One measure of complexity is taking a look at what methods and properties are available on each library’s node type.\n\n\n\nLibrary\nClass Name\nMethods\nPublic Properties\n\n\n\n\nBeautiful Soup\nbs4.element.Tag\n69\n39\n\n\nlxml\nlxml.html.HtmlElement\n43\n15\n\n\nparsel\nparsel.selector.Selector\n11\n6\n\n\nselectolax\nselectolax.parser.Node\n11\n21\n\n\n\nThis is a somewhat arbitrary measure, but illustrates that parsel and selectolax are concise APIs, perhaps at the cost of some functionality.\nMost of the methods and properties that Beautiful Soup provides are for navigating the DOM, and it has a lot of them. When Beautiful Soup came onto the scene, most scrapers did a lot more DOM traversal as XPath and CSS selector implementations were not as mature as they are today.\n\n\n6.2.3 Documentation\nBeautiful Soup has very comprehensive documentation. It has a nice quick start guide and then detailed examples of its numerous features. One thing that becomes obvious when comparing it to the others is that it has a lot of features, it has a large API for modifying the tree, and dozens of methods for various types of navigation (e.g. next_sibling, next_siblings, next_element, next_elements all exist, with the same for previous and each being slightly different from its peers).\nAs the most widely-used there’s also the advantage of a large community of users and a lot of examples online, but I’d temper that by noting that a large number of examples are old and use outdated APIs.\n\n\n\n\n\n\nThe pitfalls of popularity\n\n\n\nIn some ways, Beautiful Soup is a victim of its own success here. Popular libraries tend to accumulate features over time, and it would break backwards compatibility to remove them. With a library as widely used as Beautiful Soup, that can be a significant barrier to change.\nPerhaps there will someday be a Beautiful Soup 5 that offers a simplified API.\n\n\nlxml also has incredibly detailed documentation. The documentation site covers all of the features of lxml, which is a large library that contains many features unrelated to HTML parsing. It is a bit better if you limit your search to the lxml.html module, which is the module that contains the HTML parsing features. Though you may need to look at other parts of the documentation to understand some of the concepts, the documentation for lxml.html is fairly concise and covers most of what you’d need to know.\nparsel has a very concise API, and the documentation reflects that. Consisting primarily of a Usage page and an API reference.\nThe documentation would probably benefit from more examples, especially since parsel’s small API might leave some users wondering where features they’ve come to rely upon in other libraries are. A few more examples of how to replicate common tasks in other libraries would be helpful.\nselectolax is another very small API. Like parsel it mainly concerns itself with a small set of methods and properties available on a node type. The documentation is purely-module based and does not include any kind of tutorial or usage guide.\nOne would hope as the library matures that it will add more documentation, but for now it is a bit bare."
  },
  {
    "objectID": "parsing/index.html#speed-comparison",
    "href": "parsing/index.html#speed-comparison",
    "title": "6  Parsing HTML",
    "section": "6.3 Speed Comparison",
    "text": "6.3 Speed Comparison\nWhen talking about performance it makes sense to be realistic about the fact that speed is rarely the most important part of choosing a library for HTML parsing.\nAs we’ll see, most scrapers will be limited by the time spent making network requests, not the actual parsing of the HTML. While this is generally true, it is still good to understand the relative performance of these libraries. We’ll also take a look at when the performance of the parsers can have a significant impact on the performance of your scraper.\nTo compare these libraries, I wrote a series of benchmarks to evaluate the performance of the libraries.\n\n6.3.1 Parsing HTML\nThe initial parse of the HTML is likely the most expensive part of the scraping process. This benchmark measures the time it takes to parse the HTML using each library.\n\nExample 6.1: Parsing HTML\n\nlxml.htmlBeautiful SoupSelectolaxparsel\n\n\nroot = lxml.html.fromstring(html)\n\n\nroot = BeautifulSoup(html, 'lxml')\n    # or 'html.parser' or 'html5lib'\n\n\nroot = selectolax.parser.HTMLParser(html)\n# or selectolax.lexbor.LexborParser(html)\n\n\nroot = parsel.Selector(html)\n\n\n\n\nResults\n\n\n\nload_dom\n\n\n\n\n\nimplementation\naverage_time\nnormalized\n\n\n\n\nlxml.html\n0.09 s\n4x\n\n\nParsel\n0.09 s\n4x\n\n\nBeautifulSoup[html.parser]\n1.27 s\n51x\n\n\nBeautifulSoup[html5lib]\n2.47 s\n98x\n\n\nBeautifulSouplxml\n0.92 s\n37x\n\n\nSelectolax[modest]\n0.03 s\n1x\n\n\nSelectolax[lexbor]\n0.02 s\n1x\n\n\n\nSelectolax is the winner here, both engines performed about 4x faster than lxml.html. Parsel, as expected, was about the same speed as lxml.html since it is a thin wrapper around it. BeautifulSoup was much slower, even when using lxml as the parser, it was about 10x slower than lxml.html alone. html5lib was about 20x slower than lxml.html, and nearly 100x slower than Selectolax.\n\n\n\n\n\n\nAside: Smaller Pages\n\n\n\nIn an earlier draft of the benchmarks, I used a smaller page to test the parsers. The results were similar, but not as dramatic:\n\nTaking a look at a graph with just html5test, it is clear the relative speeds are about the same between the different test pages.\nParsing this page is so much faster than the larger more complex pages used for the rest of the tests that it basically disappeared on all graphs.\n\n\n\n\n\n\n6.3.2 Extracting Links\nThis benchmark uses each library to find all <a> tags with an href attribute. This is a common task for scrapers and given the number of links on the two test pages, should be a good test of the libraries capabilities. The libraries have different ways of doing this, so I used the most natural way for each library based on their documentation.\n\nExample 6.2: Extracting Links (Natural)\n\nlxml.htmlBeautiful SoupSelectolaxParsel\n\n\n# in lxml, XPath is the native way to do this\nlinks = root.xpath('//a[@href]')\n\n\n# in BeautifulSoup, you'd typically use find_all\nlinks = root.find_all('a', href=True)\n\n\n# Selectolax is essentially a CSS Selector implementation\nlinks = root.css('a[href]')\n\n\n# Parsel is a wrapper around lxml, so we'll use xpath\nlinks = root.xpath('//a[@href]')\n\n\n\n\nResults\n\n\n\nlinks_natural\n\n\n\n\n\nimplementation\naverage_time\nnormalized\n\n\n\n\nlxml.html\n0.0241\n11x\n\n\nParsel\n0.0469\n21x\n\n\nBeautifulSoup[html.parser]\n0.0999\n44x\n\n\nBeautifulSoup[html5lib]\n0.0998\n45x\n\n\nBeautifulSoup[lxml]\n0.101\n44x\n\n\nSelectolax[modest]\n0.00228\n1x\n\n\nSelectolax[lexbor]\n0.00236\n1x\n\n\n\nOnce again, Selectolax is in the lead. lxml and parsel are close, with parsel’s overhead adding a bit of time. BeautifulSoup is again very slow, it looks to be essentially the same speed regardless of parser. This suggests that once the DOM is parsed, BeautifulSoup is using its native methods for finding nodes, making it slower than a wrapper like parsel that takes advantage of lxml’s underlying speed.\nFurthermore, the three BeautifulSoup implementations are virtually identical in speed. This was interesting, it looks like BeautifulSoup is likely using its own implementation of find_all instead of taking advantage of lxml’s faster alternatives.\n(It was verified that all implementations gave the same count of links.)\n\n\n\n\n6.3.3 Extracting Links (CSS)\nI wanted to take a look at another way of getting the same data, in part to see if it’d level the playing field at all. Not all of the libraries support the same features, but all do support CSS selectors. We’ll be querying for the same data as before, but this time with CSS selectors.\n\n\n\n\n\n\nTip\n\n\n\nFor lxml to support this feature, it needs the cssselect library installed.\n\n\n\nExample 6.3: Extracting Links (CSS)\n\nlxml.htmlBeautifulSoupSelectolaxParsel\n\n\nlinks = root.cssselect('a[href]')\n\n\nlinks = root.select('a[href]')\n\n\nlinks = root.css('a[href]')\n\n\nlinks = root.css('a[href]')\n\n\n\n\nResults\n\n\n\nlinks_css\n\n\n\n\n\nimplementation\naverage_time\nnormalized\n\n\n\n\nlxml.html\n0.0176\n8x\n\n\nParsel\n0.0397\n19x\n\n\nBeautifulSoup[html.parser]\n0.181\n86x\n\n\nBeautifulSoup[html5lib]\n0.207\n99x\n\n\nBeautifulSoup[lxml]\n0.183\n88x\n\n\nSelectolax[modest]\n0.00210\n1x\n\n\nSelectolax[lexbor]\n0.00233\n1x\n\n\n\nThese results didn’t change much, the main difference is that BeautifulSoup got about twice as slow.\nThis did show that CSS Selectors are just as fast in lxml as XPath which is good news if you prefer using them.\n(It was verified that all implementations gave the same count of links.)\n\n\n\n\n6.3.4 Counting Elements\nFor this benchmark we’ll walk the DOM tree and count the number of elements. DOM Traversal is just about the worst way to get data out of HTML, but sometimes it is necessary.\n\nparsel\n`parsel` doesn't support direct DOM traversal.  It is possible to get child elements using XPath or CSS selectors, but it is drastically slower\nand didn't feel like a fair comparison since it isn't an intended use case.\n\nIt is also possible to use `parsel` to get the underlying `lxml` object and use that to traverse the DOM. If you are using `parsel` \nand need to do DOM traversal, this is the recommended approach.\n\n\nExample 6.4: Gathering All Elements\n\nlxml.htmlBeautiful SoupSelectolaxParsel\n\n\nall_elements = [e for e in root.iter()]\n\n\n# BeautifulSoup includes text nodes, which need to be excluded\nall_elements = [e for e in root.recursiveChildGenerator() if isinstance(e, Tag)]\n\n\nall_elements = [e for e in root.iter()]\n\n\n# Parsel doesn't support DOM traversal, but here's an \n# example of how to get the underlying lxml object\nall_elements = [e for e in root.root.iter()]\n\n\n\n\n\n\n\nimplementation\naverage_time\nnormalized\n\n\n\n\nlxml.html\n0.0281\n1.3x\n\n\nBeautifulSoup[html.parser]\n0.0229\n1.1x\n\n\nBeautifulSoup[html5lib]\n0.0248\n1.2x\n\n\nBeautifulSoup[lxml]\n0.0221\n1.04x\n\n\nSelectolax[modest]\n0.0212\n1.0x\n\n\nSelectolax[lexbor]\n0.0239\n1.1x\n\n\n\nThe variance here is the lowest of any of the benchmarks. All implementations need to do roughly the same work, traversing an already-built tree of HTML nodes in Python. lxml.html is actually the slowest here, but it seems unlikely node-traversal will be a bottleneck in any case.\n\n\n\n6.3.5 Extracting Text\nFor this benchmark, we’ll use each parser’s built in text extraction function to extract the text from the pages. These functions extract all of the text from a node and it’s descendants and are useful for things like extracting large blocks of plain text with some markup.\n\n\n\n\n\n\nTip\n\n\n\nparsel does not have an equivalent function, favoring a different approach to text extraction.\n\n\nThese methods are used to extract all of the text from a block of HTML. This is useful for things like extracting large blocks of plain text with some markup.\nFor this benchmark in particular, we’ll extract text from each of the <ul> tags on the page.\n\nExample 6.5: Extracting Text\n\n\nResults\n\n\n\n\nimplementation\naverage_time\nnormalized\n\n\n\n\nlxml.html\n0.00938\n1x\n\n\nBeautifulSoup[html.parser]\n0.0508\n5x\n\n\nBeautifulSoup[html5lib]\n0.0536\n6x\n\n\nBeautifulSoup[lxml]\n0.0506\n5x\n\n\nSelectolax[modest]\n0.0250\n3x\n\n\nSelectolax[lexbor]\n0.0237\n2x\n\n\n\nHere lxml is the clear winner. With fewer <ul> elements on the page, selectolax keeps up, but with the pyindex example the difference becomes more clear.\nAdditionally, BeautifulSoup[html.parser] and BeautifulSouplxml get different results than the rest:\n\n\n\nLibraries\nSize of result for ‘asha_bhosle’\nSize of result for ‘pyindex’\n\n\n\n\nlxml.html, html5lib, and selectolax\n2,282\n740,069\n\n\nBeautifulSoup[html.parser] and BeautifulSouplxml\n2,270\n565,339\n\n\n\nThis is a surprising result, and I’m not sure what’s going on here yet.\nI’d expected different parse trees, but html5lib For the pyindex example it is notable that html5lib and lxml.html are finding about 200,000 more characters than the other parsers. It’s also quite strange that BeautifulSoup’s lxml parser is finding the same number of characters as the html.parser, and not lxml.html.\nI expect the next section where we look at flexibility will shed some light on this.\n\n\n\n\n6.3.6 Real World Scrape\nSo far we’ve been looking at very simple benchmarks of common methods. It seems clear that lxml.html is the fastest, but how much would that speed matter in a real world scenario?\nTo simulate a real world scrape, we’ll compose a scrape from the pieces we’ve already done:\n\nParse the Python documentation index as a start page.\nFor each link on the page, parse the page the link points to. (Note: The index contains many links to the same page, we’ll parse each page each time it is encountered to simulate having many more actual pages.)\nOn each of those pages, we’ll perform 3 tasks:\n\n\nExtract the text from the root element.\nCount the number of elements on the page by walking the DOM.\nCount the spans on the page using CSS selectors.\n\nThis is a fair simulacrum of the work that a real scrape would do. All in all our mock scrape hits 11,824 pages, a moderately sized scrape.\nAnd of course, as before, all of this will be done using local files so no actual network requests will be made. An initial run will warm an in-memory cache, so disk I/O will not be a factor either.\n\nResults\n\n\n\nParser\nTime (s)\nPages/s\n\n\n\n\nlxml\n266\n44\n\n\nBeautifulSoup[html.parser]\n2,292\n5\n\n\nBeautifulSoup[html5lib]\n4,575\n3\n\n\nBeautifulSoup[lxml]\n1,694\n7\n\n\nSelectolax[modest]\n211\n56\n\n\nSelectolax[lexbor]\n274\n43\n\n\n\n!!! note\nParsel is excluded here because it does not support all the methods used in the benchmark. Since it allows you to use `lxml` under the hood, and the speed was otherwise comparable to `lxml.html`, it is fair to assume it would be comparable to `lxml.html` in this benchmark as well.\nAs is no surprise at this point, Selectolax and lxml.html are the clear winners here with no significant difference between them.\nWhile the exact amount will vary depending on the specific parsers compared, it is fair to say the C-based libraries are about an order of magnitude faster.\nIf you are able to make more than ~10 requests/second, you might find that BeautifulSoup becomes a bottleneck.\nLet’s take a look at how this plays out as we increase the number of requests per second:\n\nAs you increase the number of requests per second that you’re able to obtain, the amount of the time spent in the parser increases. As you can see, by 10 requests per second, BeautifulSoup is taking more than half the time, and by 20 requests, it is taking ~80%.\nTo contrast, lxml.html and selectolax are able to keep up with the increase in requests per second, unlikely to be the bottleneck until you are making 50+ requests per second."
  },
  {
    "objectID": "parsing/index.html#memory-comparison",
    "href": "parsing/index.html#memory-comparison",
    "title": "6  Parsing HTML",
    "section": "6.4 Memory Comparison",
    "text": "6.4 Memory Comparison\nFinally, let’s take a look at how much memory each parser uses while handling the following files:\n\n\n\nExample\nBytes\nTags\n\n\n\n\nasha_bhosle\n1,132,673\n~38,450\n\n\npyindex\n1,683,137\n~34,950\n\n\nhtml5test\n18,992\n218\n\n\n\nThis is somewhat difficult to measure, as the memory usage of an object is not easily accessible from Python. I used memray to measure a sample piece of code that loaded each parser and parsed the sample pages.\n\nThese results have a lot of interesting things to say about the parsers.\nFirst, BeautifulSoup is typically the least-memory efficient. This is probably not surprising, but it is surprising to see that there is a definite memory tax for using it with the lxml.html parser. This is particularly interesting since lxml.html is the most-memory efficient parser in each test.\nparsel performs very well here, with seemingly minimal overhead on top of it’s underlying lxml.html parser.\nselectolax looks good, sitting at the midway point between lxml.html and BeautifulSoup. It struggled however with the html5test page, included here with 10x and 100x repetitions to allow for comparison.\nIt’s interesting to see that selectolax does so poorly here. It’s possible that there is a fixed minimum of memory that selectolax uses for each page, and that the html5test page is so small that it is not able to take advantage of that minimum. In practice this shouldn’t be an issue, as typically only a single page would be loaded at a time, but it still seems worth noting as an unexpected result."
  },
  {
    "objectID": "parsing/index.html#does-performance-matter",
    "href": "parsing/index.html#does-performance-matter",
    "title": "6  Parsing HTML",
    "section": "6.5 Does Performance Matter?",
    "text": "6.5 Does Performance Matter?\nOne one hand, performance isn’t going to make or break your scrape. If you’re scraping a small number of pages, or are dealing with a slow site or rate limit, the difference between the fastest and slowest parsers is going to be negligible.\nIn practice, the real payoffs of using a faster parser are going to be felt the most during development of larger scrapers. If you’re using a local cache while scraping (and I hope you are), your requests per second are nearly limitless. This means that the speed of your parser is going to be the limiting factor in how fast you can iterate on your scrape.\nIn a 1,000 page scrape from cache of pages similar to our final benchmark, a full trial run would take less than 15 seconds while a full trial run with html5lib.parser would take nearly 3 minutes. At 10,000 pages the difference between the shortest and longest is almost half an hour.\nMemory usage might also matter to you, if you are running your scraper on a small VPS or have unusually complex pages, memory usage could be a factor and that’s another place where lxml.html shines.\nTODO: check numbers for these paragraphs w/ final results"
  },
  {
    "objectID": "parsing/index.html#bad-html",
    "href": "parsing/index.html#bad-html",
    "title": "6  Parsing HTML",
    "section": "6.6 Bad HTML",
    "text": "6.6 Bad HTML\nWe saw in the performance comparison that the results of counting particular nodes differed somewhat between parsers.\nThis mostly happens because there are differences in how they handle malformed HTML.\nIn a strict language like XML, forgetting to close a tag or containing some unescaped characters is treated as a syntax error. The parser is expected to stop parsing and report the error.\nBrowsers are much more forgiving, and will often attempt to “fix” the HTML before rendering it. That means according to certain heuristics, tags can be omitted, unescaped characters can be treated as text, and so on. If you’ve written more than a few scrapers you’ve likely encountered some, “creative” HTML, and this is core to why that is.\nOur parsers attempt to do the same thing, but do not always agree in their assumptions.\nA missing closing tag, or in some cases a single character typo, can cause a parser to fail to correctly parse the rest of the document. When this happens the scraper author is left with the choice to either fall back to crude techniques to “fix” the HTML, or to use another parser.\nTo evaluate the different parsers, we’ll look at a few examples of bad HTML and see how they handle it. This is far from comprehensive, but should give a sense of how the parsers handle common issues.\nTODO: write examples"
  },
  {
    "objectID": "parsing/index.html#conclusions",
    "href": "parsing/index.html#conclusions",
    "title": "6  Parsing HTML",
    "section": "6.7 Conclusions",
    "text": "6.7 Conclusions\nThere’s no clear winner among these. Much of it will come down to developer preference, and the needs of the project.\n\n6.7.1 Beautiful Soup\nAs the most widely used library, it is easy to find examples and documentation online. It is very feature-rich, including features like DOM modification outside the scope of this comparison.\nThe API is large and can be confusing to new users, with the often favored node-search based approach being very specific to Beautiful Soup as opposed to things like XPath & CSS selectors that are more broadly used in scrapers.\nIn many ways Beautiful Soup feels like a victim of its own success. It has a lot of legacy code and features that are no longer as relevant, but I’d imagine the sheer number of users makes it difficult to make breaking changes even if the author’s wanted to.\nFor a new project, I’d probably look at the other libraries first, but if a team was already using Beautiful Soup it might not make sense to switch unless they were running into performance issues. As the benchmarks indicated, even if they were already using the lxml backend, there is a lot to be gained by switching to lxml or Selectolax.\n\n\n6.7.2 lxml\nA powerful & underrated library, lxml.html wins major points for being the only one to support XPath and CSS selectors natively. It has a DOM traversal API that can be used similarly to Beautiful Soup’s node-search based approach if desired.\nLike Beautiful Soup, it has a somewhat sprawling API that contains dozens of methods most people will never need. Many of these are part of the ElementTree API, which is a DOM traversal API that is not specific to HTML parsing.\nThe most commonly-cited downside is that it relies upon C libraries, which can make installation and deployment more difficult on some platforms. I find with modern package managers this is less of an issue than it used to be, but it is still a valid concern.\nAs fast as it is, it is also exceedingly unlikely to become a performance bottleneck in most projects.\nIt’s a solid all-around choice.\n\n\n6.7.3 parsel\nparsel is a nice idea, a thin wrapper around lxml that focuses on providing a simple API for XPath and CSS selectors. It is a good idea, but using it leaves something to be desired. Its API is very small, perhaps going too far in the other direction.\nIt has an API that is quite different from the others, which some might find very pleasant and others might dislike.\nAs a wrapper around lxml it has the same advantages and disadvantages, with minimal overhead. Of course, if installing C libraries is a concern, it will be an issue here too.\n\n\n6.7.4 selectolax\nI was completely unfamiliar with Selectolax before starting this comparison and came away impressed. The API is small and easy to learn, but with a few convenience functions that I found nice to have compared to parsel’s approach.\nThe main downside I see is that it does not have native XPath support. While CSS selectors are more widely-used, XPath is very powerful and it is nice to have the option to use it when using lxml.html or parsel.\nselectolax of course also depends upon C libraries, and newer/unproven ones at that. That’d be a bit of a concern if I were evaluating it for a long-term project that would need to be maintained for years, but I’ll probably be giving it a try on a personal project in the near future."
  },
  {
    "objectID": "parsing/index.html#environment",
    "href": "parsing/index.html#environment",
    "title": "6  Parsing HTML",
    "section": "6.8 Environment",
    "text": "6.8 Environment\nAll benchmarks were evaluated on a 2021 MacBook Pro with an Apple M1 Pro.\n\n\n\nComponent\nVersion\n\n\n\n\nPython\n3.10.7 (installed via pyenv)\n\n\nBeautifulSoup\n4.11.1\n\n\ncchardet\n2.1.7\n\n\ncssselect\n1.2.0\n\n\nhtml5lib\n1.1\n\n\nlxml\n4.9.1\n\n\nselectolax\n0.3.11\n\n\n\nAccording to the Beautiful Soup docs installing cchardet is recommended for performance. These tests were run with cchardet installed to ensure a fair comparison, though it did not make a noticable difference in performance in these cases.\nThe sample pages referenced in the benchmarks are:\n\nPython Documentation Full Index - A fairly large page with lots of links.\nList of 2021-22 NBA Transactions - A very large Wikipedia page with a huge table.\nList of Hindi songs recorded by Asha Bhosle - At the time of writing, the largest Wikipedia page.\nHTML5 Test Page - A moderately sized page with lots of HTML5 features.\n\nAll source code for these experiments is in scraping-experiments."
  },
  {
    "objectID": "other-libraries.html#validation",
    "href": "other-libraries.html#validation",
    "title": "7  Other Libraries",
    "section": "7.1 Validation",
    "text": "7.1 Validation"
  },
  {
    "objectID": "other-libraries.html#data-storage",
    "href": "other-libraries.html#data-storage",
    "title": "7  Other Libraries",
    "section": "7.2 Data Storage",
    "text": "7.2 Data Storage"
  },
  {
    "objectID": "other-libraries.html#browser-automation",
    "href": "other-libraries.html#browser-automation",
    "title": "7  Other Libraries",
    "section": "7.3 Browser Automation",
    "text": "7.3 Browser Automation"
  },
  {
    "objectID": "selectors/index.html#css-selectors",
    "href": "selectors/index.html#css-selectors",
    "title": "Appendix A — CSS & XPath Selectors",
    "section": "A.1 CSS Selectors",
    "text": "A.1 CSS Selectors\n\nA.1.1 What is CSS?\nIf you’ve written HTML you are probably somewhat familiar with CSS. CSS, or Cascading Style Sheets, is a language for describing the presentation of HTML.\nA very simple style sheet might look something like:\nbody {\n  background-color: white;\n  color: black;\n}\na.link {\n  color: blue;\n}\nThis would make the entire body element white, with black text, and all links with the class “link” blue.\nEach statement above has two parts, the selector and the properties. The selector is the part that determines which elements the style will be applied to (e.g. the <body> element), and then the part within the curly braces determines the properties that will be applied to those elements (e.g. background-color: white;).\nCSS provides a powerful syntax for selecting elements to be styled. Websites can select elements that match very specific criteria so that (for instance) every third paragraph in an article is styled differently, or all links in a particular section have a distinct style.\nWhen we use CSS Selectors for scraping, we are leveraging this powerful syntax to pick one or more elements out of our parsed HTML tree.\nHere is an example HTML document:\n<html>\n  <body>\n    <div id=\"first\" class=\"block\">\n        <ul>\n            <li>One</li>\n            <li>Two</li>\n            <li>Three</li>\n        </ul>\n        <p class=\"inner\">After the list</p>\n    </div>\n    <div id=\"second\" class=\"block\">\n        <div id=\"inner\">\n            <p>Some text</p>\n        </div>\n    </div>\n  <body>\n</html>\n\n\nA.1.2 Basic Selectors\nBy Tag\nLet’s say someone wanted to get a list of all of the <li> elements in the document.\nThe CSS selector for this would be li. A bare tag name will match all elements of that type.\nBy Class\nIf we wanted to get a list of all of the elements with the class block, we could use the selector .block. The . is used to indicate that we are selecting by class. (I remember this by reminding myself that in most programming languages we access class attributes with a ..)\nIf the class is used on multiple tags, like “inner” is in the example above, then the selector will match all of those elements.\nYou can combine selection by tag and by class with a selector like p.inner. This will match all <p> elements with the class inner, but not the <div> element with the same class.\nBy ID\nTo select by the id attribute, you use a # instead of a .. For example, to get the div with the id first, you would use the selector #first.\nIDs are meant to be unique within HTML documents, so you typically do not need to combine this with a tag, but it is possible to do so if you need to. (e.g. div#first)\nBy Other Attribute\nWhile id and class are the most common attributes and are treated specially by CSS, you can select by any attribute using special attribute selectors.\n\n\n\n\n\n\n\nAttribute Selector\nDescription\n\n\n\n\n[attr]\nSelects all elements with the attribute attr.\n\n\n[attr=val]\nSelects all elements with the attribute attr with value val.\n\n\n[attr~=val]\nSelects all elements with the attribute attr where one of the (space-separated) values is val.\n\n\n[attr^=val]\nSelects all elements where attr starts with val.\n\n\n[attr$=val]\nSelects all elements with attr ends with val.\n\n\n[attr*=val]\nSelects all elements with attr contains val.\n\n\n\n\n\nA.1.3 Combinators\nYou can combine selectors to select elements that match more than one criteria.\n\n\n\nCombinator\nDescription\n\n\n\n\nA B\nSelects all B elements that are descendants of A.\n\n\nA > B\nSelects all B elements that are children of A.\n\n\nA + B\nSelects all B elements that are immediately preceded by A.\n\n\nA ~ B\nSelects all B elements that are preceded by A.\n\n\n\nA and B can be any other CSS selector, for example:\n\n.block #inner will select all elements with the id inner that are descendants of elements with the class block.\nul > li will select all <p> elements that are children of a <div>.\n\n\n\nA.1.4 Psuedo-Classes\nPsuedo-classes are special selectors that select elements based on their state.\n\n\n\n\n\n\n\nPsuedo-Class\nDescription\n\n\n\n\n:first-child\nSelects all elements that are the first child of their parent.\n\n\n:last-child\nSelects all elements that are the last child of their parent.\n\n\n:nth-child(n)\nSelects all elements that are the nth child of their parent.\n\n\n:only-child\nSelects all elements that are the only child of their parent.\n\n\n\nOthers may be available as well, depending on the CSS selector engine you are using.\nFor cssselect, which powers lxml and parsel’s CSS selector support you can visit https://cssselect.readthedocs.io/en/latest/#supported-selectors for more details."
  },
  {
    "objectID": "selectors/index.html#xpath-selectors",
    "href": "selectors/index.html#xpath-selectors",
    "title": "Appendix A — CSS & XPath Selectors",
    "section": "A.2 XPath Selectors",
    "text": "A.2 XPath Selectors\nXPath is a language designed for selecting elements in XML documents.\nSince HTML is a close cousin to XML, it is possible to use XPath syntax against an HTML document.\nXPath describes a means of navigating from a starting point in the document to the desired element(s).\n\nA.2.1 Starting Point\nWhen you use an XPath selector, you are starting from a particular node in the document.\nWhen using lxml.html or parsel for example you typically parse the entire HTML document, so you are starting from the root node, which is the <html> element.\nIf you have that element in a node named root, you can use root.xpath() to evaluate XPath expressions using that as the starting node.\nAs you navigate the tree, you might use other nodes as a starting point. For instance, you find a <div> element that contains the content you need, and you want to select all of the <a> elements that are children of that <div>.\nHere are some examples:\n\n\n\n\n\n\n\nXPath\nDescription\n\n\n\n\n//a\nSelects all <a> elements anywhere in the document.\n\n\n.//a\nSelects all <a> elements anywhere within the current node.\n\n\n./a\nSelects all <a> elements that are immediate children of the current node.\n\n\n../a\nSelects all <a> elements that are children of the parent of the current node. (siblings)\n\n\n\nThese XPath expressions will return different results depending on the starting point.\n\n\nA.2.2 Location Steps\nXPath makes it possible to do a fairly complex navigation of the parse tree using a syntax called location steps.\nAn XPath like //div/p/a will select all <a> elements that are descendants of a <p> element that is a descendant of a <div> element. Each piece between the slashes is known as a “location step”.\nA location step is in the form axis::node_type[predicate], only node_type is required.\nThe examples above just use the node type portion. Node types are the name of a tag (e.g. div, a, tr), or * to match all elements.\n\nPredicates\nThe predicate portion of a location step allows filtering of the elements that match the node type.\nSelecting by Attribute\nYou can select elements by attribute using syntax like //div[@id=\"first\"]. This will select all <div> elements with the id attribute set to first.\nSimilarly, //div[@class=\"block\"] will select all <div> elements with the class attribute set to block.\nUnlike CSS, there is no special syntax for id and class, all attributes can be selected in the same manner. //div[@attr=val] will select all <div> elements with the attribute attr set to val.\nUseful predicates\nNot all predicates are attribute selectors.\n[1] selects the first element matched by the slashed portion of the XPath (e.g. //div[1] selects the first <div> element in the document). (Note: XPath is 1-indexed not 0-indexed.)\n[last()] selects the last element matched by the slashed portion of the XPath (e.g. //div[last()] selects the last <div> element in the document).\n./li[position() < 4] selects the first three elements that match ./li.\n//a[contains(@href, \"pdf\")] matches all <a> tags where the href attribute contains “pdf”.\ntext() matches the text content of the current node. //a[text()=\"Next Page\"] matches all <a> tags where the text content is “Next Page”.\n\n\nAxes\nAxes in XPath allow for selection on relationships to the current node.\nTo use an axis, you precede the node portion of the XPath with the axis name, followed by ::.\nFor example, //p/ancestor::div will select all <div> elements that are ancestors of a <p> element.\nSome of the most useful axes:\n\nancestor selects all ancestors of the current node.\nancestor-or-self selects all ancestors of the current node, and the current node itself.\nchild selects all children of the current node.\ndescendant selects all descendants of the current node.\nfollowing-sibling selects all siblings of the current node that come after it.\npreceding-sibling selects all siblings of the current node that come before it.\n\n\n\n\nA.2.3 Caveat: Class Selectors in XPath\nThere’s a common gotcha that arises when using XPath to select elements by class.\nCSS treats an element like <div class=\"abc xyz\"> as having two classes, abc and xyz.\nOne might think then that the equivalent of the CSS selector div.abc.xyz would look like: //div[@class=\"abc\" and @class=\"xyz\"]. This however will not work, because in XPath all attributes are strings, and @class is a space-separated list of classes. You could match using //div[@class=\"abc xyz\"] but that would only match if that is the order, whereas CSS selectors are not order-dependent.\nIf you are doing a lot of matching on classes, CSS selectors are probably the more robust choice."
  },
  {
    "objectID": "selectors/index.html#quick-reference",
    "href": "selectors/index.html#quick-reference",
    "title": "Appendix A — CSS & XPath Selectors",
    "section": "A.3 Quick Reference",
    "text": "A.3 Quick Reference\n\n\n\n\n\n\n\n\nCSS Selector\nXPath Selector\nDescription\n\n\n\n\ndiv\n//div\nSelects all div elements.\n\n\n#xyz\n//*[@id=\"xyz\"]\nSelects an element with the id ‘xyz’.\n\n\n.xyz\n//*[@class=\"xyz\"]\nSelects an element with the class ‘xyz’.\n\n\ndiv.xyz\n//div[@class=\"xyz\"]\nSelects all div elements with the class ‘xyz’.\n\n\ndiv > p\n//div/p\nSelects all p elements that are children of a div element.\n\n\ndiv p\n//div//p\nSelects all p elements that are descendants of a div element.\n\n\ndiv + p\n//div/following-sibling::p[1]\nSelects the first p element that is a sibling of a div element.\n\n\ndiv ~ p\n//div/following-sibling::p\nSelects all p elements that are siblings of a div element."
  }
]